# ü§ñ The Agentic Architect: A Self-Study Repository on LLM Agents

## üó∫Ô∏è Navigation Menu
| **Chapter** | **Description** |
| :--- | :--- |
| [1. Foundations](#1-01-foundations-llm-agent-agentic-system) | Core concepts of LLMs, Agents, and Agentic Systems. |
| [2. Techniques](#2-02-techniques-prompt-engineering-rag-and-fine-tuning) | Practical methods: Prompt Engineering, RAG, and LLM Fine-Tuning. |
| [3. Memory & Context](#3-03-context--memory-management) | Strategies for managing agent memory and context. |
| [4. Frameworks](#4-04-frameworks-code-study) | Deep dives into agent frameworks like LangChain/LangGraph and AutoGen. |
| [5. Projects](#5-05-projects) | Complete, working examples of agents built. |
| [6. References](#6-06-references) | Curated list of papers, courses, and resources. |

---

## üåü Project Overview
Welcome to **The Agentic Architect**, my public repository dedicated to deep-diving into the theory and implementation of **Large Language Model (LLM) Agents**.

This project serves as a structured self-study journal to master the concepts from foundational LLM principles through advanced agentic system design, covering key techniques like RAG, fine-tuning, and robust memory management.

---

## üéØ Learning Objectives & Focus Areas

| Focus Area | Key Concepts Covered | Status |
| :--- | :--- | :--- |
| **Foundations** | LLM Architecture Intro, Agent Definition (P-P-A-R loop), Agentic System vs. Single Agent. | üèóÔ∏è In Progress |
| **Techniques** | Prompt Engineering (ReAct, CoT), RAG Pipeline, Parameter-Efficient Fine-Tuning (LoRA). | üèóÔ∏è In Progress |
| **Context & Memory** | Short-Term vs. Long-Term Memory, Context Window Management, Semantic Search. | üí° Planned |
| **Frameworks** | Code studies and comparisons of LangChain/LangGraph, CrewAI, AutoGen. | üí° Planned |
| **Projects** | Complete, deployable examples of specialized agents (e.g., Research Agent, Code Agent). | üí° Planned |

---

## üìñ Repository Chapters

### 1. 01-Foundations: LLM, Agent, Agentic System
* **Introduction to LLMs:** A high-level overview of the Transformer architecture and pre-training.
* **What is an Agent?** Defining the core **Perceive-Plan-Act-Reflect** loop .
* **Agentic Systems:** Differentiation between single-agent tools and multi-agent teams.

### 2. 02-Techniques: Prompt Engineering, RAG, and Fine-Tuning
* #### **Prompt Engineering**
    * Notebooks and examples demonstrating **Chain-of-Thought (CoT)** and the critical **Reason and Act (ReAct)** prompting patterns.
* #### **Retrieval Augmented Generation (RAG)**
    * Code walkthroughs covering document loading, chunking, embedding, vector storage (using libraries like `ChromaDB`), and the final generation step. .
* #### **LLM Fine-Tuning**
    * Focus on efficient methods like **LoRA (Low-Rank Adaptation)** for instruction tuning.

### 3. 03-Context & Memory Management
* Explorations into managing the agent's state: simple buffer memory vs. advanced semantic memory using vector stores (long-term memory).
* Techniques for **context window optimization** (e.g., summary, compression).

### 4. 04-Frameworks: Code Study
* **LangChain / LangGraph:** Studies on tool calling, chains, and building state-machine workflows.
* **CrewAI / AutoGen:** Examples of defining roles, tasks, and collaboration protocols for multi-agent systems.

### 5. 05-Projects
* **Example Agent:** A self-correcting research agent.
* **Example Agent:** A code generation and debugging assistant.

### 6. 06-References
A curated list of essential papers, courses, and influential projects that inform the content of this repository.

‚û°Ô∏è **[References.md](References.md)**: Find key academic papers (e.g., ReAct, LoRA), influential blogs, and recommended courses here.

---

## ü§ù Contribution & Feedback
This is a personal self-study project, but constructive feedback or suggestions for topics to explore are always welcome!

---

*Last Updated: YYYY-MM-DD*